{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2380e3b-c270-4e0c-80de-b999a2b78474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "import pandas_profiling as pdp\n",
    "import sweetviz as sv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pygam import s, LinearGAM\n",
    "import category_encoders as ce\n",
    "from typing import List, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ptitprince\n",
    "from matplotlib_venn import venn2\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino Maru Gothic Pro\"\n",
    "\n",
    "from common_module.utils.plot import (\n",
    "    plot_venn,\n",
    "    plot_histogram,\n",
    "    plot_raincloud,\n",
    "    plot_heatmap,\n",
    "    plot_histgram_for_valid,\n",
    "    plot_lineargam,\n",
    "    plot_feature_importance_for_valid\n",
    ")\n",
    "from common_module.utils.summarize import (\n",
    "    get_pandas_profiling,\n",
    "    get_sweetviz_report\n",
    ")\n",
    "from common_module.utils.preprocessor import (\n",
    "    OrdinalEncodingBlock,\n",
    "    CountEncodingBlock,\n",
    "    GroupingBlock\n",
    ")\n",
    "from common_module.utils.format_df import format_df\n",
    "from common_module.utils.utils import reduce_mem_usage\n",
    "\n",
    "from probspace_usstock.modules.config_manager import ConfigManager\n",
    "from probspace_usstock.modules.features import (\n",
    "    create_company_features,\n",
    "    calc_Symbol_lag_features,\n",
    "    calc_Symbol_lag_log_features,\n",
    "    # calc_List_lag_features,\n",
    "    create_percentile_flg_columns,\n",
    "    create_ymd_features\n",
    ")\n",
    "from probspace_usstock.modules.preprocessor import (\n",
    "    melt_dataframe,\n",
    "    create_base_dataframe\n",
    ")\n",
    "from probspace_usstock.modules.train_module import TrainLGBModule\n",
    "from probspace_usstock.modules.predict import make_pred_df\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37e1172-2ba4-4068-b18c-5c68276731f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 3279)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7007, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3278, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read\n",
    "df_train_origin = pd.read_csv(\"../input/train_data.csv\", encoding=\"utf-8-sig\")\n",
    "df_company_list_origin = pd.read_csv(\"../input/company_list.csv\", encoding=\"utf-8-sig\")\n",
    "df_sub_template = pd.read_csv(\"../input/submission_template.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# shape\n",
    "display(df_train_origin.shape)\n",
    "display(df_company_list_origin.shape)\n",
    "display(df_sub_template.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86b2496-cdd5-4309-a2d3-70aceb645cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/setting.yaml\"\n",
    "config_manager = ConfigManager(config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade7e4d7-5c09-4616-b8fe-eda48dfd60ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>IPOyear</th>\n",
       "      <th>Sector</th>\n",
       "      <th>List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XXII</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consumer Non-Durables</td>\n",
       "      <td>AMEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAX</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMEX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol  IPOyear                 Sector  List\n",
       "0   XXII      NaN  Consumer Non-Durables  AMEX\n",
       "1    FAX   1986.0                    NaN  AMEX"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6991, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# company関連の特徴量を先に作ってListをマージしたい\n",
    "df_company_features = create_company_features(\n",
    "    input=df_company_list_origin,\n",
    "    duplicate_companies=config_manager.duplicate_companies\n",
    ")\n",
    "display(df_company_features.head(2))\n",
    "display(df_company_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9119e2b5-6317-40fa-b3a9-92ef3aeb8395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>List</th>\n",
       "      <th>stock_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011/11/13</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>55.942184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011/11/20</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>55.978844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol    List  stock_price\n",
       "0  2011/11/13   VGSH  NASDAQ    55.942184\n",
       "1  2011/11/20   VGSH  NASDAQ    55.978844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_base = melt_dataframe(input=df_train_origin)\n",
    "df_base = create_base_dataframe(input=df_base, df_company=df_company_features)\n",
    "\n",
    "display(df_base.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ec2631-ad6a-4057-85c1-7f8a677b73a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011/11/13</td>\n",
       "      <td>2011</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011/11/20</td>\n",
       "      <td>2011</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  year  month  day  week_of_month  week_of_year\n",
       "0  2011/11/13  2011     11   13              2            45\n",
       "1  2011/11/20  2011     11   20              3            46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>stock_price_log1p</th>\n",
       "      <th>stock_price_log1p_1s</th>\n",
       "      <th>stock_price_log1p_2s</th>\n",
       "      <th>stock_price_log1p_3s</th>\n",
       "      <th>stock_price_log1p_4s</th>\n",
       "      <th>increase_stock_price_log1p_1s2s</th>\n",
       "      <th>increase_stock_price_log1p_2s3s</th>\n",
       "      <th>increase_stock_price_log1p_3s4s</th>\n",
       "      <th>stock_price_log1p_1s_4r_mean</th>\n",
       "      <th>stock_price_log1p_1s_4r_median</th>\n",
       "      <th>stock_price_log1p_1s_4r_max</th>\n",
       "      <th>stock_price_log1p_1s_4r_min</th>\n",
       "      <th>stock_price_log1p_1s_8r_mean</th>\n",
       "      <th>stock_price_log1p_1s_8r_median</th>\n",
       "      <th>stock_price_log1p_1s_8r_max</th>\n",
       "      <th>stock_price_log1p_1s_8r_min</th>\n",
       "      <th>stock_price_log1p_1s_12r_mean</th>\n",
       "      <th>stock_price_log1p_1s_12r_median</th>\n",
       "      <th>stock_price_log1p_1s_12r_max</th>\n",
       "      <th>stock_price_log1p_1s_12r_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011/11/13</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>4.042036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011/11/20</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>4.042680</td>\n",
       "      <td>4.042036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol  stock_price_log1p  stock_price_log1p_1s  \\\n",
       "0  2011/11/13   VGSH           4.042036                   NaN   \n",
       "1  2011/11/20   VGSH           4.042680              4.042036   \n",
       "\n",
       "   stock_price_log1p_2s  stock_price_log1p_3s  stock_price_log1p_4s  \\\n",
       "0                   NaN                   NaN                   NaN   \n",
       "1                   NaN                   NaN                   NaN   \n",
       "\n",
       "   increase_stock_price_log1p_1s2s  increase_stock_price_log1p_2s3s  \\\n",
       "0                              NaN                              NaN   \n",
       "1                              NaN                              NaN   \n",
       "\n",
       "   increase_stock_price_log1p_3s4s  stock_price_log1p_1s_4r_mean  \\\n",
       "0                              NaN                           NaN   \n",
       "1                              NaN                           NaN   \n",
       "\n",
       "   stock_price_log1p_1s_4r_median  stock_price_log1p_1s_4r_max  \\\n",
       "0                             NaN                          NaN   \n",
       "1                             NaN                          NaN   \n",
       "\n",
       "   stock_price_log1p_1s_4r_min  stock_price_log1p_1s_8r_mean  \\\n",
       "0                          NaN                           NaN   \n",
       "1                          NaN                           NaN   \n",
       "\n",
       "   stock_price_log1p_1s_8r_median  stock_price_log1p_1s_8r_max  \\\n",
       "0                             NaN                          NaN   \n",
       "1                             NaN                          NaN   \n",
       "\n",
       "   stock_price_log1p_1s_8r_min  stock_price_log1p_1s_12r_mean  \\\n",
       "0                          NaN                            NaN   \n",
       "1                          NaN                            NaN   \n",
       "\n",
       "   stock_price_log1p_1s_12r_median  stock_price_log1p_1s_12r_max  \\\n",
       "0                              NaN                           NaN   \n",
       "1                              NaN                           NaN   \n",
       "\n",
       "   stock_price_log1p_1s_12r_min  \n",
       "0                           NaN  \n",
       "1                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>is_05_percentile_stock_price</th>\n",
       "      <th>is_95_percentile_stock_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011/11/13</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011/11/20</td>\n",
       "      <td>VGSH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol  is_05_percentile_stock_price  \\\n",
       "0  2011/11/13   VGSH                             0   \n",
       "1  2011/11/20   VGSH                             0   \n",
       "\n",
       "   is_95_percentile_stock_price  \n",
       "0                             0  \n",
       "1                             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 1.79 s, total: 1min 23s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_ymd = create_ymd_features(input=df_base)\n",
    "df_Symbol_lag = calc_Symbol_lag_log_features(input=df_base)\n",
    "# df_Symbol_lag = calc_Symbol_lag_features(input=df_base)\n",
    "# df_List_lag = reduce_mem_usage(calc_List_lag_features(input=df_Symbol_lag))\n",
    "df_flg = create_percentile_flg_columns(input=df_Symbol_lag)\n",
    "\n",
    "display(df_ymd.head(2))\n",
    "display(df_Symbol_lag.head(2))\n",
    "# display(df_List_lag.head(2))\n",
    "display(df_flg.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829aab42-4e81-4502-a124-d78e38372866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 654 ms, total: 2.35 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# _df_base = df_base[pd.to_datetime(df_base[\"Date\"]) < pd.to_datetime(\"2013-01-01\")].copy()\n",
    "\n",
    "df_merge = pd.merge(df_base, df_company_features.drop(\"List\", axis=1), how=\"left\", on=\"Symbol\")\n",
    "df_merge = pd.merge(df_merge, df_ymd, how=\"left\", on=\"Date\")\n",
    "df_merge = pd.merge(df_merge, df_Symbol_lag, how=\"left\", on=[\"Date\", \"Symbol\"])\n",
    "# df = pd.merge(df, df_List_lag, how=\"left\", on=[\"Date\", \"List\"])\n",
    "df_merge = pd.merge(df_merge, df_flg, how=\"left\", on=[\"Date\", \"Symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac17b23-9cd6-4067-9d35-6ce36b677a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1376760, 33)\n",
      "(1337424, 33)\n",
      "Memory usage of dataframe is 346.93 MB\n",
      "Memory usage after optimization is: 84.35 MB\n",
      "Decreased by 75.7%\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1337424 entries, 12 to 1376759\n",
      "Data columns (total 33 columns):\n",
      " #   Column                           Non-Null Count    Dtype   \n",
      "---  ------                           --------------    -----   \n",
      " 0   Date                             1337424 non-null  category\n",
      " 1   Symbol                           1337424 non-null  category\n",
      " 2   List                             1335792 non-null  category\n",
      " 3   stock_price                      1334146 non-null  float16 \n",
      " 4   IPOyear                          441864 non-null   float16 \n",
      " 5   Sector                           1114656 non-null  category\n",
      " 6   year                             1337424 non-null  int16   \n",
      " 7   month                            1337424 non-null  int8    \n",
      " 8   day                              1337424 non-null  int8    \n",
      " 9   week_of_month                    1337424 non-null  int8    \n",
      " 10  week_of_year                     1337424 non-null  int8    \n",
      " 11  stock_price_log1p                1334146 non-null  float16 \n",
      " 12  stock_price_log1p_1s             1337424 non-null  float16 \n",
      " 13  stock_price_log1p_2s             1337424 non-null  float16 \n",
      " 14  stock_price_log1p_3s             1337424 non-null  float16 \n",
      " 15  stock_price_log1p_4s             1337424 non-null  float16 \n",
      " 16  increase_stock_price_log1p_1s2s  1337424 non-null  float16 \n",
      " 17  increase_stock_price_log1p_2s3s  1337424 non-null  float16 \n",
      " 18  increase_stock_price_log1p_3s4s  1337424 non-null  float16 \n",
      " 19  stock_price_log1p_1s_4r_mean     1337424 non-null  float16 \n",
      " 20  stock_price_log1p_1s_4r_median   1337424 non-null  float16 \n",
      " 21  stock_price_log1p_1s_4r_max      1337424 non-null  float16 \n",
      " 22  stock_price_log1p_1s_4r_min      1337424 non-null  float16 \n",
      " 23  stock_price_log1p_1s_8r_mean     1337424 non-null  float16 \n",
      " 24  stock_price_log1p_1s_8r_median   1337424 non-null  float16 \n",
      " 25  stock_price_log1p_1s_8r_max      1337424 non-null  float16 \n",
      " 26  stock_price_log1p_1s_8r_min      1337424 non-null  float16 \n",
      " 27  stock_price_log1p_1s_12r_mean    1337424 non-null  float16 \n",
      " 28  stock_price_log1p_1s_12r_median  1337424 non-null  float16 \n",
      " 29  stock_price_log1p_1s_12r_max     1337424 non-null  float16 \n",
      " 30  stock_price_log1p_1s_12r_min     1337424 non-null  float16 \n",
      " 31  is_05_percentile_stock_price     1337424 non-null  int8    \n",
      " 32  is_95_percentile_stock_price     1337424 non-null  int8    \n",
      "dtypes: category(4), float16(22), int16(1), int8(6)\n",
      "memory usage: 84.4 MB\n"
     ]
    }
   ],
   "source": [
    "# NaNがある期間は学習に使用しない\n",
    "print(df_merge.shape)\n",
    "df_merge = df_merge[df_merge[\"stock_price_log1p_1s_12r_mean\"].notnull()]\n",
    "# df_merge = df_merge[df_merge[\"stock_price_1s_12r_mean\"].notnull()]\n",
    "print(df_merge.shape)\n",
    "df_merge = reduce_mem_usage(df_merge)\n",
    "df_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919a57ca-92e6-44c6-9ad7-6577246040d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = format_df(df_merge, config_manager.feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8557fba-e705-4f74-aa45-56974971a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(input: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
    "    df_train = input[\n",
    "        (pd.to_datetime(input[\"Date\"]) >= pd.to_datetime(config.train_period[0])) &\n",
    "        (pd.to_datetime(input[\"Date\"]) <= pd.to_datetime(config.train_period[1]))\n",
    "    ].copy()\n",
    "    df_val = input[\n",
    "        (pd.to_datetime(input[\"Date\"]) >= pd.to_datetime(config.valid_period[0])) &\n",
    "        (pd.to_datetime(input[\"Date\"]) <= pd.to_datetime(config.valid_period[1]))\n",
    "    ].copy()\n",
    "    df_test = input[\n",
    "        (pd.to_datetime(input[\"Date\"]) >= pd.to_datetime(config.test_period[0])) &\n",
    "        (pd.to_datetime(input[\"Date\"]) <= pd.to_datetime(config.test_period[1]))\n",
    "    ].copy()\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df_train, df_val, df_test = split_dataframe(input=df_merge, config=config_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ff036e-e74b-4ea9-ae9c-f23151babea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lgb_weight(input: pd.DataFrame) -> pd.DataFrame:\n",
    "    return 1 / input[\"stock_price_log1p_1s_4r_mean\"].replace(0, 1) * 10\n",
    "\n",
    "weight_train = calc_lgb_weight(df_train)\n",
    "weight_val = calc_lgb_weight(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "343a1bbf-7605-4a28-be34-71eba128ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "y_train = df_train[config_manager.pred_cols].copy()\n",
    "y_val = df_val[config_manager.pred_cols].copy()\n",
    "X_train = df_train[config_manager.feature_columns.keys()].drop(config_manager.primary_key, axis=1).copy()\n",
    "X_val = df_val[config_manager.feature_columns.keys()].drop(config_manager.primary_key, axis=1).copy()\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, weight=weight_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, weight=weight_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac1924-6def-45f2-ba42-de3605ff61f1",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38cf2df4-6e1d-4dd7-8b68-8d39f9bd3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna経由でLightGBMをインポート\n",
    "import optuna.integration.lightgbm as LGB_optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b35a025b-0009-45d9-8f20-4222e7d07738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-18 18:01:22,302]\u001b[0m A new study created in memory with name: no-name-15b874e3-3bdd-4ec4-af99-9093efa64eab\u001b[0m\n",
      "\n",
      "\n",
      "  0%|                                                                                                                           | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: inf:   0%|                                                                                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:   0%|                                                                                    | 0/7 [00:21<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  14%|##########8                                                                 | 1/7 [00:21<02:07, 21.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:01:43,520]\u001b[0m Trial 0 finished with value: 0.06073659305911132 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.06073659305911132.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  14%|##########8                                                                 | 1/7 [00:21<02:07, 21.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[431]\tvalid_0's rmse: 0.0607366\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  14%|##########8                                                                 | 1/7 [00:36<02:07, 21.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  29%|#####################7                                                      | 2/7 [00:36<01:27, 17.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:01:58,359]\u001b[0m Trial 1 finished with value: 0.06116450751246555 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.06073659305911132.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  29%|#####################7                                                      | 2/7 [00:36<01:27, 17.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's rmse: 0.0611645\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  29%|#####################7                                                      | 2/7 [00:50<01:27, 17.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  43%|################################5                                           | 3/7 [00:50<01:03, 15.92s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:02:12,442]\u001b[0m Trial 2 finished with value: 0.06078816304795276 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.06073659305911132.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060737:  43%|################################5                                           | 3/7 [00:50<01:03, 15.92s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[210]\tvalid_0's rmse: 0.0607882\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  43%|################################5                                           | 3/7 [01:03<01:03, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  57%|###########################################4                                | 4/7 [01:03<00:45, 15.01s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:02:26,071]\u001b[0m Trial 3 finished with value: 0.06072682918518341 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 3 with value: 0.06072682918518341.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  57%|###########################################4                                | 4/7 [01:03<00:45, 15.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's rmse: 0.0607268\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  57%|###########################################4                                | 4/7 [01:44<00:45, 15.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  71%|######################################################2                     | 5/7 [01:44<00:48, 24.41s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:03:07,153]\u001b[0m Trial 4 finished with value: 0.06145017445197061 and parameters: {'feature_fraction': 0.4}. Best is trial 3 with value: 0.06072682918518341.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060727:  71%|######################################################2                     | 5/7 [01:44<00:48, 24.41s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[743]\tvalid_0's rmse: 0.0614502\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060646:  71%|######################################################2                     | 5/7 [02:04<00:48, 24.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060646:  86%|#################################################################1          | 6/7 [02:04<00:22, 22.82s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:03:26,891]\u001b[0m Trial 5 finished with value: 0.06064550710174979 and parameters: {'feature_fraction': 1.0}. Best is trial 5 with value: 0.06064550710174979.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060646:  86%|#################################################################1          | 6/7 [02:04<00:22, 22.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[323]\tvalid_0's rmse: 0.0606455\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.060542:  86%|#################################################################1          | 6/7 [02:22<00:22, 22.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.060542: 100%|############################################################################| 7/7 [02:22<00:00, 21.27s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:03:44,964]\u001b[0m Trial 6 finished with value: 0.060541702865575905 and parameters: {'feature_fraction': 0.8}. Best is trial 6 with value: 0.060541702865575905.\u001b[0m\n",
      "feature_fraction, val_score: 0.060542: 100%|############################################################################| 7/7 [02:22<00:00, 20.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:   0%|                                                                                         | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:   0%|                                                                                         | 0/20 [00:13<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:   5%|####                                                                             | 1/20 [00:13<04:15, 13.45s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:03:58,442]\u001b[0m Trial 7 finished with value: 0.060951551894640583 and parameters: {'num_leaves': 247}. Best is trial 7 with value: 0.060951551894640583.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:   5%|####                                                                             | 1/20 [00:13<04:15, 13.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's rmse: 0.0609516\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:   5%|####                                                                             | 1/20 [00:24<04:15, 13.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  10%|########1                                                                        | 2/20 [00:24<03:41, 12.28s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:04:09,906]\u001b[0m Trial 8 finished with value: 0.06095865175527261 and parameters: {'num_leaves': 51}. Best is trial 7 with value: 0.060951551894640583.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  10%|########1                                                                        | 2/20 [00:24<03:41, 12.28s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's rmse: 0.0609587\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  10%|########1                                                                        | 2/20 [00:40<03:41, 12.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  15%|############1                                                                    | 3/20 [00:40<03:55, 13.85s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:04:25,625]\u001b[0m Trial 9 finished with value: 0.0610201993886985 and parameters: {'num_leaves': 44}. Best is trial 7 with value: 0.060951551894640583.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  15%|############1                                                                    | 3/20 [00:40<03:55, 13.85s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[262]\tvalid_0's rmse: 0.0610202\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  15%|############1                                                                    | 3/20 [00:53<03:55, 13.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  20%|################2                                                                | 4/20 [00:53<03:34, 13.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:04:38,309]\u001b[0m Trial 10 finished with value: 0.06092665175580627 and parameters: {'num_leaves': 186}. Best is trial 10 with value: 0.06092665175580627.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  20%|################2                                                                | 4/20 [00:53<03:34, 13.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's rmse: 0.0609267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  20%|################2                                                                | 4/20 [01:08<03:34, 13.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  25%|####################2                                                            | 5/20 [01:08<03:30, 14.04s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:04:53,510]\u001b[0m Trial 11 finished with value: 0.06091861498322337 and parameters: {'num_leaves': 223}. Best is trial 11 with value: 0.06091861498322337.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  25%|####################2                                                            | 5/20 [01:08<03:30, 14.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 0.0609186\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  25%|####################2                                                            | 5/20 [01:24<03:30, 14.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  30%|########################3                                                        | 6/20 [01:24<03:26, 14.77s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:05:09,676]\u001b[0m Trial 12 finished with value: 0.060848828672111416 and parameters: {'num_leaves': 243}. Best is trial 12 with value: 0.060848828672111416.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  30%|########################3                                                        | 6/20 [01:24<03:26, 14.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 0.0608488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.195766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  30%|########################3                                                        | 6/20 [01:46<03:26, 14.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  35%|############################3                                                    | 7/20 [01:46<03:39, 16.92s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:05:31,021]\u001b[0m Trial 13 finished with value: 0.06077311389477257 and parameters: {'num_leaves': 245}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  35%|############################3                                                    | 7/20 [01:46<03:39, 16.92s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's rmse: 0.0607731\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  35%|############################3                                                    | 7/20 [02:03<03:39, 16.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  40%|################################4                                                | 8/20 [02:03<03:24, 17.04s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:05:48,319]\u001b[0m Trial 14 finished with value: 0.06097477407748243 and parameters: {'num_leaves': 157}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  40%|################################4                                                | 8/20 [02:03<03:24, 17.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 0.0609748\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  40%|################################4                                                | 8/20 [02:25<03:24, 17.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  45%|####################################4                                            | 9/20 [02:26<03:26, 18.80s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:06:11,003]\u001b[0m Trial 15 finished with value: 0.0611094726194716 and parameters: {'num_leaves': 69}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  45%|####################################4                                            | 9/20 [02:26<03:26, 18.80s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[298]\tvalid_0's rmse: 0.0611095\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  45%|####################################4                                            | 9/20 [02:44<03:26, 18.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  50%|########################################                                        | 10/20 [02:44<03:08, 18.80s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:06:29,810]\u001b[0m Trial 16 finished with value: 0.06078401541677366 and parameters: {'num_leaves': 37}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  50%|########################################                                        | 10/20 [02:44<03:08, 18.80s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's rmse: 0.060784\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  50%|########################################                                        | 10/20 [03:12<03:08, 18.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  55%|############################################                                    | 11/20 [03:12<03:14, 21.59s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:06:57,732]\u001b[0m Trial 17 finished with value: 0.06104843369628767 and parameters: {'num_leaves': 116}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  55%|############################################                                    | 11/20 [03:12<03:14, 21.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's rmse: 0.0610484\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  55%|############################################                                    | 11/20 [03:43<03:14, 21.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  60%|################################################                                | 12/20 [03:43<03:15, 24.41s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:07:28,595]\u001b[0m Trial 18 finished with value: 0.0611346468965821 and parameters: {'num_leaves': 101}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  60%|################################################                                | 12/20 [03:43<03:15, 24.41s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's rmse: 0.0611346\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  60%|################################################                                | 12/20 [03:58<03:15, 24.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  65%|####################################################                            | 13/20 [03:58<02:31, 21.63s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:07:43,806]\u001b[0m Trial 19 finished with value: 0.06096827242190385 and parameters: {'num_leaves': 185}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  65%|####################################################                            | 13/20 [03:58<02:31, 21.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's rmse: 0.0609683\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  65%|####################################################                            | 13/20 [04:35<02:31, 21.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  70%|########################################################                        | 14/20 [04:35<02:37, 26.22s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:08:20,649]\u001b[0m Trial 20 finished with value: 0.06106337954550571 and parameters: {'num_leaves': 9}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  70%|########################################################                        | 14/20 [04:35<02:37, 26.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's rmse: 0.0610658\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's rmse: 0.0610634\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  70%|########################################################                        | 14/20 [04:58<02:37, 26.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  75%|############################################################                    | 15/20 [04:58<02:05, 25.08s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:08:43,089]\u001b[0m Trial 21 finished with value: 0.06947351124830056 and parameters: {'num_leaves': 2}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  75%|############################################################                    | 15/20 [04:58<02:05, 25.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's rmse: 0.0694749\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's rmse: 0.0694735\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  75%|############################################################                    | 15/20 [05:15<02:05, 25.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  80%|################################################################                | 16/20 [05:15<01:30, 22.63s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:09:00,036]\u001b[0m Trial 22 finished with value: 0.06125510315423279 and parameters: {'num_leaves': 89}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  80%|################################################################                | 16/20 [05:15<01:30, 22.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's rmse: 0.0612551\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  80%|################################################################                | 16/20 [05:31<01:30, 22.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  85%|####################################################################            | 17/20 [05:31<01:02, 20.83s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:09:16,667]\u001b[0m Trial 23 finished with value: 0.06100728473591469 and parameters: {'num_leaves': 144}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  85%|####################################################################            | 17/20 [05:31<01:02, 20.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's rmse: 0.0610073\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  85%|####################################################################            | 17/20 [05:51<01:02, 20.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  90%|########################################################################        | 18/20 [05:51<00:41, 20.51s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:09:36,429]\u001b[0m Trial 24 finished with value: 0.06089342864285273 and parameters: {'num_leaves': 202}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  90%|########################################################################        | 18/20 [05:51<00:41, 20.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's rmse: 0.0608934\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  90%|########################################################################        | 18/20 [06:06<00:41, 20.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542:  95%|############################################################################    | 19/20 [06:06<00:18, 18.87s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:09:51,493]\u001b[0m Trial 25 finished with value: 0.06097477407748243 and parameters: {'num_leaves': 157}. Best is trial 13 with value: 0.06077311389477257.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  95%|############################################################################    | 19/20 [06:06<00:18, 18.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 0.0609748\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.060542:  95%|############################################################################    | 19/20 [06:25<00:18, 18.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.060542: 100%|################################################################################| 20/20 [06:25<00:00, 18.86s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:10:10,321]\u001b[0m Trial 26 finished with value: 0.060541702865575905 and parameters: {'num_leaves': 31}. Best is trial 26 with value: 0.060541702865575905.\u001b[0m\n",
      "num_leaves, val_score: 0.060542: 100%|################################################################################| 20/20 [06:25<00:00, 19.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:   0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:   0%|                                                                                            | 0/10 [00:16<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  10%|########4                                                                           | 1/10 [00:16<02:31, 16.79s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:10:27,132]\u001b[0m Trial 27 finished with value: 0.06123384926511288 and parameters: {'bagging_fraction': 0.6624624490180899, 'bagging_freq': 1}. Best is trial 27 with value: 0.06123384926511288.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  10%|########4                                                                           | 1/10 [00:16<02:31, 16.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[235]\tvalid_0's rmse: 0.0612338\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  10%|########4                                                                           | 1/10 [00:36<02:31, 16.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  20%|################8                                                                   | 2/10 [00:36<02:29, 18.66s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:10:47,088]\u001b[0m Trial 28 finished with value: 0.061293473203898156 and parameters: {'bagging_fraction': 0.8036629096797159, 'bagging_freq': 3}. Best is trial 27 with value: 0.06123384926511288.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  20%|################8                                                                   | 2/10 [00:36<02:29, 18.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's rmse: 0.0612935\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  20%|################8                                                                   | 2/10 [01:06<02:29, 18.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  30%|#########################2                                                          | 3/10 [01:06<02:45, 23.62s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:11:16,618]\u001b[0m Trial 29 finished with value: 0.060955288655373766 and parameters: {'bagging_fraction': 0.8945402700063474, 'bagging_freq': 3}. Best is trial 29 with value: 0.060955288655373766.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  30%|#########################2                                                          | 3/10 [01:06<02:45, 23.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's rmse: 0.0609553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  30%|#########################2                                                          | 3/10 [01:25<02:45, 23.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  40%|#################################6                                                  | 4/10 [01:25<02:10, 21.83s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:11:35,716]\u001b[0m Trial 30 finished with value: 0.0609528906203437 and parameters: {'bagging_fraction': 0.9486638625908898, 'bagging_freq': 5}. Best is trial 30 with value: 0.0609528906203437.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  40%|#################################6                                                  | 4/10 [01:25<02:10, 21.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's rmse: 0.0609529\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  40%|#################################6                                                  | 4/10 [02:00<02:10, 21.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  50%|##########################################                                          | 5/10 [02:00<02:13, 26.63s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:12:10,861]\u001b[0m Trial 31 finished with value: 0.060902954186669654 and parameters: {'bagging_fraction': 0.951447959633712, 'bagging_freq': 4}. Best is trial 31 with value: 0.060902954186669654.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  50%|##########################################                                          | 5/10 [02:00<02:13, 26.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's rmse: 0.060903\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.334179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  50%|##########################################                                          | 5/10 [02:33<02:13, 26.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  60%|##################################################4                                 | 6/10 [02:33<01:54, 28.67s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:12:43,483]\u001b[0m Trial 32 finished with value: 0.060904526645598235 and parameters: {'bagging_fraction': 0.9561184777365423, 'bagging_freq': 3}. Best is trial 31 with value: 0.060902954186669654.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  60%|##################################################4                                 | 6/10 [02:33<01:54, 28.67s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's rmse: 0.0609045\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  60%|##################################################4                                 | 6/10 [03:03<01:54, 28.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  70%|##########################################################8                         | 7/10 [03:03<01:27, 29.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:13:13,861]\u001b[0m Trial 33 finished with value: 0.061103862080330126 and parameters: {'bagging_fraction': 0.8163870401144167, 'bagging_freq': 4}. Best is trial 31 with value: 0.060902954186669654.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  70%|##########################################################8                         | 7/10 [03:03<01:27, 29.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[309]\tvalid_0's rmse: 0.0611039\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  70%|##########################################################8                         | 7/10 [03:41<01:27, 29.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  80%|###################################################################2                | 8/10 [03:41<01:03, 31.97s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:13:51,711]\u001b[0m Trial 34 finished with value: 0.060657293610538336 and parameters: {'bagging_fraction': 0.9829409917054063, 'bagging_freq': 2}. Best is trial 34 with value: 0.060657293610538336.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  80%|###################################################################2                | 8/10 [03:41<01:03, 31.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[417]\tvalid_0's rmse: 0.0606573\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  80%|###################################################################2                | 8/10 [04:08<01:03, 31.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542:  90%|###########################################################################6        | 9/10 [04:08<00:30, 30.50s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:14:18,958]\u001b[0m Trial 35 finished with value: 0.0608838927912509 and parameters: {'bagging_fraction': 0.9961967527672703, 'bagging_freq': 6}. Best is trial 34 with value: 0.060657293610538336.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  90%|###########################################################################6        | 9/10 [04:08<00:30, 30.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[298]\tvalid_0's rmse: 0.0608839\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.060542:  90%|###########################################################################6        | 9/10 [04:26<00:30, 30.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.060542: 100%|###################################################################################| 10/10 [04:26<00:00, 26.66s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:14:37,036]\u001b[0m Trial 36 finished with value: 0.06102893758846236 and parameters: {'bagging_fraction': 0.6669792266017097, 'bagging_freq': 2}. Best is trial 34 with value: 0.060657293610538336.\u001b[0m\n",
      "bagging, val_score: 0.060542: 100%|###################################################################################| 10/10 [04:26<00:00, 26.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's rmse: 0.0610289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                           | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:   0%|                                                                             | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:   0%|                                                                             | 0/6 [00:22<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  17%|###########5                                                         | 1/6 [00:22<01:50, 22.03s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:14:59,084]\u001b[0m Trial 37 finished with value: 0.06054170286557591 and parameters: {'feature_fraction': 0.784}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  17%|###########5                                                         | 1/6 [00:22<01:50, 22.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605417\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  17%|###########5                                                         | 1/6 [00:40<01:50, 22.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  33%|#######################                                              | 2/6 [00:40<01:19, 19.97s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:15:17,605]\u001b[0m Trial 38 finished with value: 0.0609175640913036 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  33%|#######################                                              | 2/6 [00:40<01:19, 19.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[284]\tvalid_0's rmse: 0.0609176\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  33%|#######################                                              | 2/6 [00:56<01:19, 19.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  50%|##################################5                                  | 3/6 [00:56<00:54, 18.18s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:15:33,655]\u001b[0m Trial 39 finished with value: 0.06072682918518341 and parameters: {'feature_fraction': 0.88}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  50%|##################################5                                  | 3/6 [00:56<00:54, 18.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's rmse: 0.0607268\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  50%|##################################5                                  | 3/6 [01:11<00:54, 18.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  67%|##############################################                       | 4/6 [01:11<00:34, 17.03s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:15:48,917]\u001b[0m Trial 40 finished with value: 0.06078816304795276 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  67%|##############################################                       | 4/6 [01:11<00:34, 17.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[210]\tvalid_0's rmse: 0.0607882\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  67%|##############################################                       | 4/6 [01:27<00:34, 17.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  83%|#########################################################5           | 5/6 [01:27<00:16, 16.51s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:16:04,519]\u001b[0m Trial 41 finished with value: 0.06108120909066129 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  83%|#########################################################5           | 5/6 [01:27<00:16, 16.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[243]\tvalid_0's rmse: 0.0610812\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542:  83%|#########################################################5           | 5/6 [01:45<00:16, 16.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.060542: 100%|#####################################################################| 6/6 [01:45<00:00, 17.18s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:16:22,994]\u001b[0m Trial 42 finished with value: 0.061119631940756126 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.06054170286557591.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.060542: 100%|#####################################################################| 6/6 [01:45<00:00, 17.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[282]\tvalid_0's rmse: 0.0611196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:   0%|                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:   0%|                                                                             | 0/20 [00:22<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:   5%|###4                                                                 | 1/20 [00:22<07:10, 22.65s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:16:45,661]\u001b[0m Trial 43 finished with value: 0.060542933153292365 and parameters: {'lambda_l1': 1.0737300309707931e-08, 'lambda_l2': 4.3947785189976324e-07}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:   5%|###4                                                                 | 1/20 [00:22<07:10, 22.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:   5%|###4                                                                 | 1/20 [00:50<07:10, 22.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  10%|######9                                                              | 2/20 [00:50<07:44, 25.83s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:17:13,716]\u001b[0m Trial 44 finished with value: 0.060740732527501895 and parameters: {'lambda_l1': 0.0019038896314636919, 'lambda_l2': 5.007531175751098e-08}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  10%|######9                                                              | 2/20 [00:50<07:44, 25.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[523]\tvalid_0's rmse: 0.0607407\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  10%|######9                                                              | 2/20 [01:11<07:44, 25.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  15%|##########3                                                          | 3/20 [01:11<06:43, 23.75s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:17:35,005]\u001b[0m Trial 45 finished with value: 0.06093023297669826 and parameters: {'lambda_l1': 5.273210211232847e-06, 'lambda_l2': 0.7183894995874321}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  15%|##########3                                                          | 3/20 [01:12<06:43, 23.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[413]\tvalid_0's rmse: 0.0609302\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  15%|##########3                                                          | 3/20 [01:29<06:43, 23.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  20%|#############8                                                       | 4/20 [01:29<05:41, 21.37s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:17:52,722]\u001b[0m Trial 46 finished with value: 0.06054459976562503 and parameters: {'lambda_l1': 1.8279473943025287e-05, 'lambda_l2': 1.0869073636620201e-07}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  20%|#############8                                                       | 4/20 [01:29<05:41, 21.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  20%|#############8                                                       | 4/20 [01:46<05:41, 21.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  25%|#################2                                                   | 5/20 [01:46<04:54, 19.65s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:18:09,308]\u001b[0m Trial 47 finished with value: 0.06086509293933892 and parameters: {'lambda_l1': 0.0004551210840944232, 'lambda_l2': 2.7102600832015284e-08}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  25%|#################2                                                   | 5/20 [01:46<04:54, 19.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's rmse: 0.0608651\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  25%|#################2                                                   | 5/20 [02:05<04:54, 19.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  30%|####################7                                                | 6/20 [02:05<04:34, 19.58s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:18:28,771]\u001b[0m Trial 48 finished with value: 0.06054459958978677 and parameters: {'lambda_l1': 6.68764848625442e-08, 'lambda_l2': 2.5136374568182258e-05}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  30%|####################7                                                | 6/20 [02:05<04:34, 19.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  30%|####################7                                                | 6/20 [02:34<04:34, 19.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  35%|########################1                                            | 7/20 [02:34<04:51, 22.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:18:57,150]\u001b[0m Trial 49 finished with value: 0.060875348904419896 and parameters: {'lambda_l1': 0.04547133525197374, 'lambda_l2': 4.356050099427134e-06}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  35%|########################1                                            | 7/20 [02:34<04:51, 22.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[510]\tvalid_0's rmse: 0.0608753\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  35%|########################1                                            | 7/20 [02:53<04:51, 22.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  40%|###########################6                                         | 8/20 [02:53<04:18, 21.53s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:19:16,685]\u001b[0m Trial 50 finished with value: 0.0605446012606146 and parameters: {'lambda_l1': 1.4515575926626355e-08, 'lambda_l2': 0.001017215300747719}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  40%|###########################6                                         | 8/20 [02:53<04:18, 21.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  40%|###########################6                                         | 8/20 [03:08<04:18, 21.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  45%|###############################                                      | 9/20 [03:08<03:33, 19.42s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:19:31,462]\u001b[0m Trial 51 finished with value: 0.060758246031446764 and parameters: {'lambda_l1': 0.007591672302719021, 'lambda_l2': 7.110007158571725e-08}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  45%|###############################                                      | 9/20 [03:08<03:33, 19.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's rmse: 0.0607582\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  45%|###############################                                      | 9/20 [03:26<03:33, 19.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  50%|##################################                                  | 10/20 [03:26<03:10, 19.07s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:19:49,742]\u001b[0m Trial 52 finished with value: 0.06054460793844027 and parameters: {'lambda_l1': 1.5103873734575933e-08, 'lambda_l2': 0.0049689908923084575}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  50%|##################################                                  | 10/20 [03:26<03:10, 19.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  50%|##################################                                  | 10/20 [03:49<03:10, 19.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  55%|#####################################4                              | 11/20 [03:49<03:01, 20.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:20:12,227]\u001b[0m Trial 53 finished with value: 0.06098119246123039 and parameters: {'lambda_l1': 9.180976256218491, 'lambda_l2': 5.782568021936321e-06}. Best is trial 43 with value: 0.060542933153292365.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  55%|#####################################4                              | 11/20 [03:49<03:01, 20.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's rmse: 0.0609812\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  55%|#####################################4                              | 11/20 [04:10<03:01, 20.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  60%|########################################8                           | 12/20 [04:10<02:43, 20.42s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:20:33,362]\u001b[0m Trial 54 finished with value: 0.06054170291702797 and parameters: {'lambda_l1': 6.458146961439342e-07, 'lambda_l2': 2.543689616876014e-05}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  60%|########################################8                           | 12/20 [04:10<02:43, 20.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605417\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  60%|########################################8                           | 12/20 [04:30<02:43, 20.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  65%|############################################2                       | 13/20 [04:30<02:22, 20.38s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:20:53,629]\u001b[0m Trial 55 finished with value: 0.060544599623193474 and parameters: {'lambda_l1': 6.426653917997996e-07, 'lambda_l2': 4.1991987779424666e-05}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  65%|############################################2                       | 13/20 [04:30<02:22, 20.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  65%|############################################2                       | 13/20 [04:50<02:22, 20.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  70%|###############################################5                    | 14/20 [04:50<02:00, 20.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:21:13,120]\u001b[0m Trial 56 finished with value: 0.06083780547674663 and parameters: {'lambda_l1': 7.031933839956633e-07, 'lambda_l2': 0.022687642318649098}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  70%|###############################################5                    | 14/20 [04:50<02:00, 20.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[265]\tvalid_0's rmse: 0.0608378\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  70%|###############################################5                    | 14/20 [05:10<02:00, 20.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  75%|###################################################                 | 15/20 [05:10<01:40, 20.16s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:21:33,413]\u001b[0m Trial 57 finished with value: 0.06054459986600151 and parameters: {'lambda_l1': 2.656492998364172e-05, 'lambda_l2': 1.0329092525823671e-06}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  75%|###################################################                 | 15/20 [05:10<01:40, 20.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  75%|###################################################                 | 15/20 [05:27<01:40, 20.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  80%|######################################################4             | 16/20 [05:27<01:17, 19.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:21:50,994]\u001b[0m Trial 58 finished with value: 0.06054293344353412 and parameters: {'lambda_l1': 4.3187120143533526e-07, 'lambda_l2': 0.0001692975609267657}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  80%|######################################################4             | 16/20 [05:27<01:17, 19.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  80%|######################################################4             | 16/20 [05:42<01:17, 19.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  85%|#########################################################8          | 17/20 [05:42<00:53, 17.83s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:22:05,199]\u001b[0m Trial 59 finished with value: 0.06108198530661018 and parameters: {'lambda_l1': 1.1800799063673542e-07, 'lambda_l2': 0.1228009457165231}. Best is trial 54 with value: 0.06054170291702797.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  85%|#########################################################8          | 17/20 [05:42<00:53, 17.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[245]\tvalid_0's rmse: 0.061082\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  85%|#########################################################8          | 17/20 [06:01<00:53, 17.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  90%|#############################################################2      | 18/20 [06:01<00:36, 18.22s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:22:24,350]\u001b[0m Trial 60 finished with value: 0.06054170289039167 and parameters: {'lambda_l1': 2.0001387335139926e-06, 'lambda_l2': 4.6753493130493636e-07}. Best is trial 60 with value: 0.06054170289039167.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  90%|#############################################################2      | 18/20 [06:01<00:36, 18.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605417\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  90%|#############################################################2      | 18/20 [06:15<00:36, 18.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  95%|################################################################6   | 19/20 [06:15<00:16, 16.93s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:22:38,258]\u001b[0m Trial 61 finished with value: 0.06070174333179974 and parameters: {'lambda_l1': 7.436535860372558e-05, 'lambda_l2': 9.388779691959519}. Best is trial 60 with value: 0.06054170289039167.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  95%|################################################################6   | 19/20 [06:15<00:16, 16.93s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[207]\tvalid_0's rmse: 0.0607017\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.060542:  95%|################################################################6   | 19/20 [06:37<00:16, 16.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.060542: 100%|####################################################################| 20/20 [06:37<00:00, 18.64s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:23:00,884]\u001b[0m Trial 62 finished with value: 0.06054460114431193 and parameters: {'lambda_l1': 2.7040492113937142e-06, 'lambda_l2': 0.0009283119282906979}. Best is trial 60 with value: 0.06054170289039167.\u001b[0m\n",
      "regularization_factors, val_score: 0.060542: 100%|####################################################################| 20/20 [06:37<00:00, 19.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.0605446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                           | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:   0%|                                                                                    | 0/5 [00:18<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  20%|###############2                                                            | 1/5 [00:18<01:13, 18.44s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:23:19,344]\u001b[0m Trial 63 finished with value: 0.060808402880029415 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.060808402880029415.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  20%|###############2                                                            | 1/5 [00:18<01:13, 18.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[292]\tvalid_0's rmse: 0.0608084\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  20%|###############2                                                            | 1/5 [00:35<01:13, 18.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  40%|##############################4                                             | 2/5 [00:35<00:52, 17.58s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:23:36,324]\u001b[0m Trial 64 finished with value: 0.06113020643459949 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.060808402880029415.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  40%|##############################4                                             | 2/5 [00:35<00:52, 17.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's rmse: 0.0611302\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  40%|##############################4                                             | 2/5 [00:51<00:52, 17.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  60%|#############################################6                              | 3/5 [00:51<00:33, 16.77s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:23:52,124]\u001b[0m Trial 65 finished with value: 0.06097048694161683 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.060808402880029415.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  60%|#############################################6                              | 3/5 [00:51<00:33, 16.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's rmse: 0.0609705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  60%|#############################################6                              | 3/5 [01:04<00:33, 16.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  80%|############################################################8               | 4/5 [01:04<00:15, 15.49s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:24:05,641]\u001b[0m Trial 66 finished with value: 0.061078779847547005 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.060808402880029415.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  80%|############################################################8               | 4/5 [01:04<00:15, 15.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's rmse: 0.0610788\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4225\n",
      "[LightGBM] [Info] Number of data points in the train set: 1294810, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.442752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542:  80%|############################################################8               | 4/5 [01:17<00:15, 15.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.060542: 100%|############################################################################| 5/5 [01:17<00:00, 14.45s/it]\u001b[A\u001b[A\u001b[32m[I 2022-03-18 18:24:18,248]\u001b[0m Trial 67 finished with value: 0.061019248017073355 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.060808402880029415.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.060542: 100%|############################################################################| 5/5 [01:17<00:00, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[156]\tvalid_0's rmse: 0.0610192\n",
      "Best params: {'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'num_iterations': 1000, 'early_stopping_round': 100, 'categorical_column': [0, 2, 3, 4, 5, 6]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 固定するパラメータ\n",
    "fixed_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "# Optunaでのパラメータ探索\n",
    "model = LGB_optuna.train(\n",
    "    params=fixed_params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=val_data,\n",
    "    verbose_eval=1000,\n",
    "    early_stopping_rounds=100,\n",
    ")\n",
    "\n",
    "# 最適なパラメータの表示\n",
    "best_params = model.params\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c808e270-c70b-4445-b5b6-fa759837ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'num_iterations': 1000, 'early_stopping_round': 100, 'categorical_column': [0, 2, 3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# 最適なパラメータの表示\n",
    "best_params = model.params\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b91b9-4fdc-42ec-8354-65ae6302ba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
